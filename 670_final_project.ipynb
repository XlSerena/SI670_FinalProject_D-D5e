{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2f826d",
   "metadata": {},
   "source": [
    "# 670 Final Project: D&D Spell School Classification\n",
    "* Group members:\n",
    "    * Yufeng Song (yfsong)\n",
    "    * Lan Xu (lanxu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a6307cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "df = pd.read_csv('data/spells.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923741c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name              0\n",
       "desc              0\n",
       "higher_level    229\n",
       "material        135\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"name\", \"desc\", \"higher_level\", \"material\"]].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fec801b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = [\"name\", \"desc\", \"higher_level\", \"material\"]\n",
    "for col in text_cols:\n",
    "    df[col] = df[col].fillna(\"\")\n",
    "\n",
    "df[\"full_text\"] = (\n",
    "    df[\"name\"] + \". \" +\n",
    "    df[\"desc\"] + \" \" +\n",
    "    df[\"higher_level\"] + \" \" +\n",
    "    df[\"material\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bbbeeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[\"school_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0985b6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e2fdc78aef473eb5a98fcba9bc92b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_text_emb = model.encode(df[\"full_text\"], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e596625",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_text_emb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c9ddb",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e55968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Training Model: Logistic Regression\n",
      "Macro Precision = 0.7000\n",
      "Macro Recall    = 0.5198\n",
      "Macro F1-score  = 0.5621\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.60      0.38      0.46         8\n",
      "  conjuration       0.62      0.50      0.56        10\n",
      "   divination       1.00      0.33      0.50         6\n",
      "  enchantment       0.67      0.67      0.67         6\n",
      "    evocation       0.33      0.58      0.42        12\n",
      "     illusion       1.00      0.80      0.89         5\n",
      "   necromancy       1.00      0.40      0.57         5\n",
      "transmutation       0.38      0.50      0.43        12\n",
      "\n",
      "     accuracy                           0.52        64\n",
      "    macro avg       0.70      0.52      0.56        64\n",
      " weighted avg       0.62      0.52      0.53        64\n",
      "\n",
      "==================================================\n",
      "Training Model: Linear SVM\n",
      "Macro Precision = 0.6916\n",
      "Macro Recall    = 0.5958\n",
      "Macro F1-score  = 0.6108\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.57      0.50      0.53         8\n",
      "  conjuration       0.78      0.70      0.74        10\n",
      "   divination       1.00      0.33      0.50         6\n",
      "  enchantment       0.56      0.83      0.67         6\n",
      "    evocation       0.41      0.58      0.48        12\n",
      "     illusion       0.80      0.80      0.80         5\n",
      "   necromancy       1.00      0.60      0.75         5\n",
      "transmutation       0.42      0.42      0.42        12\n",
      "\n",
      "     accuracy                           0.58        64\n",
      "    macro avg       0.69      0.60      0.61        64\n",
      " weighted avg       0.63      0.58      0.58        64\n",
      "\n",
      "==================================================\n",
      "Training Model: RBF SVM\n",
      "Macro Precision = 0.7258\n",
      "Macro Recall    = 0.6094\n",
      "Macro F1-score  = 0.6458\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.71      0.62      0.67         8\n",
      "  conjuration       0.86      0.60      0.71        10\n",
      "   divination       0.67      0.33      0.44         6\n",
      "  enchantment       0.67      0.67      0.67         6\n",
      "    evocation       0.53      0.67      0.59        12\n",
      "     illusion       1.00      0.80      0.89         5\n",
      "   necromancy       1.00      0.60      0.75         5\n",
      "transmutation       0.37      0.58      0.45        12\n",
      "\n",
      "     accuracy                           0.61        64\n",
      "    macro avg       0.73      0.61      0.65        64\n",
      " weighted avg       0.67      0.61      0.62        64\n",
      "\n",
      "==================================================\n",
      "Training Model: Polynomial SVM (degree=3)\n",
      "Macro Precision = 0.7385\n",
      "Macro Recall    = 0.4979\n",
      "Macro F1-score  = 0.5357\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.67      0.50      0.57         8\n",
      "  conjuration       0.60      0.60      0.60        10\n",
      "   divination       1.00      0.17      0.29         6\n",
      "  enchantment       0.75      0.50      0.60         6\n",
      "    evocation       0.50      0.67      0.57        12\n",
      "     illusion       1.00      0.40      0.57         5\n",
      "   necromancy       1.00      0.40      0.57         5\n",
      "transmutation       0.39      0.75      0.51        12\n",
      "\n",
      "     accuracy                           0.55        64\n",
      "    macro avg       0.74      0.50      0.54        64\n",
      " weighted avg       0.66      0.55      0.54        64\n",
      "\n",
      "==================================================\n",
      "Training Model: Decision Tree\n",
      "Macro Precision = 0.3174\n",
      "Macro Recall    = 0.3229\n",
      "Macro F1-score  = 0.3178\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.25      0.25      0.25         8\n",
      "  conjuration       0.20      0.20      0.20        10\n",
      "   divination       0.20      0.17      0.18         6\n",
      "  enchantment       0.25      0.33      0.29         6\n",
      "    evocation       0.46      0.50      0.48        12\n",
      "     illusion       0.33      0.40      0.36         5\n",
      "   necromancy       0.40      0.40      0.40         5\n",
      "transmutation       0.44      0.33      0.38        12\n",
      "\n",
      "     accuracy                           0.33        64\n",
      "    macro avg       0.32      0.32      0.32        64\n",
      " weighted avg       0.33      0.33      0.33        64\n",
      "\n",
      "==================================================\n",
      "Training Model: Extra Trees\n",
      "Macro Precision = 0.7210\n",
      "Macro Recall    = 0.5312\n",
      "Macro F1-score  = 0.5717\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.67      0.25      0.36         8\n",
      "  conjuration       0.62      0.50      0.56        10\n",
      "   divination       1.00      0.50      0.67         6\n",
      "  enchantment       0.67      0.67      0.67         6\n",
      "    evocation       0.43      0.83      0.57        12\n",
      "     illusion       1.00      0.60      0.75         5\n",
      "   necromancy       1.00      0.40      0.57         5\n",
      "transmutation       0.38      0.50      0.43        12\n",
      "\n",
      "     accuracy                           0.55        64\n",
      "    macro avg       0.72      0.53      0.57        64\n",
      " weighted avg       0.65      0.55      0.55        64\n",
      "\n",
      "==================================================\n",
      "Training Model: Gradient Boosting\n",
      "Macro Precision = 0.4571\n",
      "Macro Recall    = 0.3594\n",
      "Macro F1-score  = 0.3702\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.17      0.12      0.14         8\n",
      "  conjuration       0.40      0.40      0.40        10\n",
      "   divination       0.20      0.17      0.18         6\n",
      "  enchantment       0.60      0.50      0.55         6\n",
      "    evocation       0.35      0.50      0.41        12\n",
      "     illusion       0.50      0.40      0.44         5\n",
      "   necromancy       1.00      0.20      0.33         5\n",
      "transmutation       0.44      0.58      0.50        12\n",
      "\n",
      "     accuracy                           0.39        64\n",
      "    macro avg       0.46      0.36      0.37        64\n",
      " weighted avg       0.42      0.39      0.38        64\n",
      "\n",
      "==================================================\n",
      "Training Model: KNN (k=5)\n",
      "Macro Precision = 0.6819\n",
      "Macro Recall    = 0.6229\n",
      "Macro F1-score  = 0.6168\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.75      0.75      0.75         8\n",
      "  conjuration       0.45      0.50      0.48        10\n",
      "   divination       1.00      0.33      0.50         6\n",
      "  enchantment       0.57      0.67      0.62         6\n",
      "    evocation       0.58      0.92      0.71        12\n",
      "     illusion       0.67      0.80      0.73         5\n",
      "   necromancy       0.60      0.60      0.60         5\n",
      "transmutation       0.83      0.42      0.56        12\n",
      "\n",
      "     accuracy                           0.62        64\n",
      "    macro avg       0.68      0.62      0.62        64\n",
      " weighted avg       0.68      0.62      0.61        64\n",
      "\n",
      "==================================================\n",
      "Training Model: Gaussian NB\n",
      "Macro Precision = 0.6259\n",
      "Macro Recall    = 0.5729\n",
      "Macro F1-score  = 0.5730\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.46      0.75      0.57         8\n",
      "  conjuration       0.60      0.60      0.60        10\n",
      "   divination       1.00      0.33      0.50         6\n",
      "  enchantment       0.60      0.50      0.55         6\n",
      "    evocation       0.55      0.50      0.52        12\n",
      "     illusion       0.80      0.80      0.80         5\n",
      "   necromancy       0.50      0.60      0.55         5\n",
      "transmutation       0.50      0.50      0.50        12\n",
      "\n",
      "     accuracy                           0.56        64\n",
      "    macro avg       0.63      0.57      0.57        64\n",
      " weighted avg       0.60      0.56      0.56        64\n",
      "\n",
      "==================================================\n",
      "Training Model: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision = 0.4270\n",
      "Macro Recall    = 0.4292\n",
      "Macro F1-score  = 0.4255\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.25      0.25      0.25         8\n",
      "  conjuration       0.31      0.40      0.35        10\n",
      "   divination       0.00      0.00      0.00         6\n",
      "  enchantment       0.80      0.67      0.73         6\n",
      "    evocation       0.31      0.42      0.36        12\n",
      "     illusion       0.60      0.60      0.60         5\n",
      "   necromancy       0.60      0.60      0.60         5\n",
      "transmutation       0.55      0.50      0.52        12\n",
      "\n",
      "     accuracy                           0.42        64\n",
      "    macro avg       0.43      0.43      0.43        64\n",
      " weighted avg       0.41      0.42      0.41        64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    # linear models\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=3000),\n",
    "    \"Linear SVM\": SVC(kernel=\"linear\"),\n",
    "\n",
    "    # kernel SVM family\n",
    "    \"RBF SVM\": SVC(kernel=\"rbf\", gamma=\"scale\", C=1.0),\n",
    "    \"Polynomial SVM (degree=3)\": SVC(kernel=\"poly\", degree=3, C=1.0),\n",
    "\n",
    "    # Trees (non-bagging and bagging variants)\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=None),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(n_estimators=300),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "\n",
    "    # Neighbors\n",
    "    \"KNN (k=5)\": KNeighborsClassifier(n_neighbors=5),\n",
    "\n",
    "    # Naive Bayes\n",
    "    \"Gaussian NB\": GaussianNB(),\n",
    "\n",
    "    # XGBoost baseline\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        eval_metric=\"mlogloss\"\n",
    "    )\n",
    "}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Training Model: {name}\")\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    \n",
    "    macro_precision = precision_score(y_test, preds, average=\"macro\")\n",
    "    macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "    macro_f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "\n",
    "    print(f\"Macro Precision = {macro_precision:.4f}\")\n",
    "    print(f\"Macro Recall    = {macro_recall:.4f}\")\n",
    "    print(f\"Macro F1-score  = {macro_f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-class performance:\")\n",
    "    print(classification_report(y_test, preds, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee021b4",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "#### Summary Table (Macro F1)\n",
    "| Model | Macro F1 |\n",
    "|--------|-----------|\n",
    "| **RBF SVM** | **0.6458** |\n",
    "| **KNN (k=5)** | **0.6168** |\n",
    "| **Linear SVM** | **0.6108** |\n",
    "| Logistic Regression | 0.5621 |\n",
    "| Extra Trees | 0.5717 |\n",
    "| Gaussian NB | 0.5730 |\n",
    "| Polynomial SVM (deg=3) | 0.5357 |\n",
    "| Gradient Boosting | 0.3702 |\n",
    "| XGBoost | 0.4255 |\n",
    "| Decision Tree | 0.3178 |\n",
    "\n",
    "#### Key Insights\n",
    "1. **Kernel SVMs outperform all other baselines**, with RBF SVM achieving the highest macro F1.\n",
    "2. **KNN is surprisingly strong**, suggesting that spells cluster cleanly in embedding space.\n",
    "3. **Linear SVM > Logistic Regression**, confirming mild linear separability.\n",
    "4. **Tree-based models consistently underperform**; dense embeddings are not well suited for axis-aligned splits.\n",
    "5. Certain schools, especially **Divination and Transmutation**, remain challenging due to semantic overlap.\n",
    "\n",
    "#### What This Means\n",
    "- The embedding space captures semantic structure, but *not all schools are equally separable*.\n",
    "- Nonlinear decision boundaries (RBF SVM) help significantly.\n",
    "- More advanced models must focus on better representing nuanced spell semantics.\n",
    "\n",
    "#### Next Steps\n",
    "1. **Tune the RBF SVM**  \n",
    "   - Grid search `C` and `gamma`  \n",
    "   - Expect significant improvement.\n",
    "\n",
    "2. **Upgrade the embedding model**  \n",
    "   - Try `all-mpnet-base-v2` or `bge-large-en-v1.5`\n",
    "   - Strongly boosts class separation.\n",
    "\n",
    "3. **Add structured features**  \n",
    "   Combine embeddings with:\n",
    "   - spell level  \n",
    "   - casting time category  \n",
    "   - duration category  \n",
    "\n",
    "4. **Try a simple neural classifier**  \n",
    "   - MLP on top of embeddings (1â€“2 layers)\n",
    "\n",
    "5. **Analyze confusion matrix**  \n",
    "   - Understand which schools consistently overlap (likely: Evocation/Transmutation/Conjuration).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
