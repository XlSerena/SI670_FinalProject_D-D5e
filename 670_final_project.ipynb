{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2f826d",
   "metadata": {},
   "source": [
    "# 670 Final Project: D&D Spell School Classification\n",
    "* Group members:\n",
    "    * Yufeng Song (yfsong)\n",
    "    * Lan Xu (lanxu)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6a6307cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T03:50:03.568947Z",
     "start_time": "2025-12-06T03:49:55.680012Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "df = pd.read_csv('data/spells.csv')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "923741c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T03:50:03.584985Z",
     "start_time": "2025-12-06T03:50:03.579621Z"
    }
   },
   "source": [
    "df[[\"name\", \"desc\", \"higher_level\", \"material\"]].isna().sum()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name              0\n",
       "desc              0\n",
       "higher_level    229\n",
       "material        135\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "4fec801b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T03:50:03.848671Z",
     "start_time": "2025-12-06T03:50:03.845106Z"
    }
   },
   "source": [
    "text_cols = [\"name\", \"desc\", \"higher_level\", \"material\"]\n",
    "for col in text_cols:\n",
    "    df[col] = df[col].fillna(\"\")\n",
    "\n",
    "df[\"full_text\"] = (\n",
    "    df[\"name\"] + \". \" +\n",
    "    df[\"desc\"] + \" \" +\n",
    "    df[\"higher_level\"] + \" \" +\n",
    "    df[\"material\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "4bbbeeba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T03:50:03.979388Z",
     "start_time": "2025-12-06T03:50:03.977358Z"
    }
   },
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[\"school_index\"])"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "0985b6e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T03:50:09.463783Z",
     "start_time": "2025-12-06T03:50:04.097783Z"
    }
   },
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_text_emb = model.encode(df[\"full_text\"], show_progress_bar=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49938fce1ba545aface7293543d460fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9e596625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T03:50:09.474393Z",
     "start_time": "2025-12-06T03:50:09.470101Z"
    }
   },
   "source": [
    "X = X_text_emb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "e88c9ddb",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e55968e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T03:50:30.295630Z",
     "start_time": "2025-12-06T03:50:09.491102Z"
    }
   },
   "source": [
    "models = {\n",
    "    # linear models\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=3000),\n",
    "    \"Linear SVM\": SVC(kernel=\"linear\"),\n",
    "\n",
    "    # kernel SVM family\n",
    "    \"RBF SVM\": SVC(kernel=\"rbf\", gamma=\"scale\", C=1.0),\n",
    "    \"Polynomial SVM (degree=3)\": SVC(kernel=\"poly\", degree=3, C=1.0),\n",
    "\n",
    "    # Trees (non-bagging and bagging variants)\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=None),\n",
    "    \"Extra Trees\": ExtraTreesClassifier(n_estimators=300),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "\n",
    "    # Neighbors\n",
    "    \"KNN (k=5)\": KNeighborsClassifier(n_neighbors=5),\n",
    "\n",
    "    # Naive Bayes\n",
    "    \"Gaussian NB\": GaussianNB(),\n",
    "\n",
    "    # XGBoost baseline\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        eval_metric=\"mlogloss\"\n",
    "    )\n",
    "}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Training Model: {name}\")\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    \n",
    "    macro_precision = precision_score(y_test, preds, average=\"macro\")\n",
    "    macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "    macro_f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "\n",
    "    print(f\"Macro Precision = {macro_precision:.4f}\")\n",
    "    print(f\"Macro Recall    = {macro_recall:.4f}\")\n",
    "    print(f\"Macro F1-score  = {macro_f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nPer-class performance:\")\n",
    "    print(classification_report(y_test, preds, target_names=label_encoder.classes_))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Training Model: Logistic Regression\n",
      "Macro Precision = 0.7000\n",
      "Macro Recall    = 0.5198\n",
      "Macro F1-score  = 0.5621\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.60      0.38      0.46         8\n",
      "  conjuration       0.62      0.50      0.56        10\n",
      "   divination       1.00      0.33      0.50         6\n",
      "  enchantment       0.67      0.67      0.67         6\n",
      "    evocation       0.33      0.58      0.42        12\n",
      "     illusion       1.00      0.80      0.89         5\n",
      "   necromancy       1.00      0.40      0.57         5\n",
      "transmutation       0.38      0.50      0.43        12\n",
      "\n",
      "     accuracy                           0.52        64\n",
      "    macro avg       0.70      0.52      0.56        64\n",
      " weighted avg       0.62      0.52      0.53        64\n",
      "\n",
      "==================================================\n",
      "Training Model: Linear SVM\n",
      "Macro Precision = 0.6916\n",
      "Macro Recall    = 0.5958\n",
      "Macro F1-score  = 0.6108\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.57      0.50      0.53         8\n",
      "  conjuration       0.78      0.70      0.74        10\n",
      "   divination       1.00      0.33      0.50         6\n",
      "  enchantment       0.56      0.83      0.67         6\n",
      "    evocation       0.41      0.58      0.48        12\n",
      "     illusion       0.80      0.80      0.80         5\n",
      "   necromancy       1.00      0.60      0.75         5\n",
      "transmutation       0.42      0.42      0.42        12\n",
      "\n",
      "     accuracy                           0.58        64\n",
      "    macro avg       0.69      0.60      0.61        64\n",
      " weighted avg       0.63      0.58      0.58        64\n",
      "\n",
      "==================================================\n",
      "Training Model: RBF SVM\n",
      "Macro Precision = 0.7258\n",
      "Macro Recall    = 0.6094\n",
      "Macro F1-score  = 0.6458\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.71      0.62      0.67         8\n",
      "  conjuration       0.86      0.60      0.71        10\n",
      "   divination       0.67      0.33      0.44         6\n",
      "  enchantment       0.67      0.67      0.67         6\n",
      "    evocation       0.53      0.67      0.59        12\n",
      "     illusion       1.00      0.80      0.89         5\n",
      "   necromancy       1.00      0.60      0.75         5\n",
      "transmutation       0.37      0.58      0.45        12\n",
      "\n",
      "     accuracy                           0.61        64\n",
      "    macro avg       0.73      0.61      0.65        64\n",
      " weighted avg       0.67      0.61      0.62        64\n",
      "\n",
      "==================================================\n",
      "Training Model: Polynomial SVM (degree=3)\n",
      "Macro Precision = 0.7385\n",
      "Macro Recall    = 0.4979\n",
      "Macro F1-score  = 0.5357\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.67      0.50      0.57         8\n",
      "  conjuration       0.60      0.60      0.60        10\n",
      "   divination       1.00      0.17      0.29         6\n",
      "  enchantment       0.75      0.50      0.60         6\n",
      "    evocation       0.50      0.67      0.57        12\n",
      "     illusion       1.00      0.40      0.57         5\n",
      "   necromancy       1.00      0.40      0.57         5\n",
      "transmutation       0.39      0.75      0.51        12\n",
      "\n",
      "     accuracy                           0.55        64\n",
      "    macro avg       0.74      0.50      0.54        64\n",
      " weighted avg       0.66      0.55      0.54        64\n",
      "\n",
      "==================================================\n",
      "Training Model: Decision Tree\n",
      "Macro Precision = 0.3417\n",
      "Macro Recall    = 0.3083\n",
      "Macro F1-score  = 0.3158\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.33      0.25      0.29         8\n",
      "  conjuration       0.15      0.20      0.17        10\n",
      "   divination       0.00      0.00      0.00         6\n",
      "  enchantment       0.38      0.50      0.43         6\n",
      "    evocation       0.57      0.67      0.62        12\n",
      "     illusion       0.33      0.20      0.25         5\n",
      "   necromancy       0.67      0.40      0.50         5\n",
      "transmutation       0.30      0.25      0.27        12\n",
      "\n",
      "     accuracy                           0.33        64\n",
      "    macro avg       0.34      0.31      0.32        64\n",
      " weighted avg       0.34      0.33      0.33        64\n",
      "\n",
      "==================================================\n",
      "Training Model: Extra Trees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lan/my_env/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  raw_prediction = X @ weights.T + intercept  # ndarray, likely C-contiguous\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: divide by zero encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: overflow encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:336: RuntimeWarning: invalid value encountered in matmul\n",
      "  grad[:, :n_features] = grad_pointwise.T @ X + l2_reg_strength * weights\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: divide by zero encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/Users/lan/my_env/lib/python3.13/site-packages/sklearn/utils/extmath.py:203: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision = 0.7356\n",
      "Macro Recall    = 0.5583\n",
      "Macro F1-score  = 0.5971\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.57      0.50      0.53         8\n",
      "  conjuration       0.67      0.60      0.63        10\n",
      "   divination       1.00      0.33      0.50         6\n",
      "  enchantment       0.75      0.50      0.60         6\n",
      "    evocation       0.43      0.83      0.57        12\n",
      "     illusion       1.00      0.80      0.89         5\n",
      "   necromancy       1.00      0.40      0.57         5\n",
      "transmutation       0.46      0.50      0.48        12\n",
      "\n",
      "     accuracy                           0.58        64\n",
      "    macro avg       0.74      0.56      0.60        64\n",
      " weighted avg       0.66      0.58      0.58        64\n",
      "\n",
      "==================================================\n",
      "Training Model: Gradient Boosting\n",
      "Macro Precision = 0.3821\n",
      "Macro Recall    = 0.3688\n",
      "Macro F1-score  = 0.3675\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.33      0.25      0.29         8\n",
      "  conjuration       0.33      0.40      0.36        10\n",
      "   divination       0.00      0.00      0.00         6\n",
      "  enchantment       0.43      0.50      0.46         6\n",
      "    evocation       0.36      0.42      0.38        12\n",
      "     illusion       0.50      0.40      0.44         5\n",
      "   necromancy       0.67      0.40      0.50         5\n",
      "transmutation       0.44      0.58      0.50        12\n",
      "\n",
      "     accuracy                           0.39        64\n",
      "    macro avg       0.38      0.37      0.37        64\n",
      " weighted avg       0.37      0.39      0.38        64\n",
      "\n",
      "==================================================\n",
      "Training Model: KNN (k=5)\n",
      "Macro Precision = 0.6819\n",
      "Macro Recall    = 0.6229\n",
      "Macro F1-score  = 0.6168\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.75      0.75      0.75         8\n",
      "  conjuration       0.45      0.50      0.48        10\n",
      "   divination       1.00      0.33      0.50         6\n",
      "  enchantment       0.57      0.67      0.62         6\n",
      "    evocation       0.58      0.92      0.71        12\n",
      "     illusion       0.67      0.80      0.73         5\n",
      "   necromancy       0.60      0.60      0.60         5\n",
      "transmutation       0.83      0.42      0.56        12\n",
      "\n",
      "     accuracy                           0.62        64\n",
      "    macro avg       0.68      0.62      0.62        64\n",
      " weighted avg       0.68      0.62      0.61        64\n",
      "\n",
      "==================================================\n",
      "Training Model: Gaussian NB\n",
      "Macro Precision = 0.6259\n",
      "Macro Recall    = 0.5729\n",
      "Macro F1-score  = 0.5730\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.46      0.75      0.57         8\n",
      "  conjuration       0.60      0.60      0.60        10\n",
      "   divination       1.00      0.33      0.50         6\n",
      "  enchantment       0.60      0.50      0.55         6\n",
      "    evocation       0.55      0.50      0.52        12\n",
      "     illusion       0.80      0.80      0.80         5\n",
      "   necromancy       0.50      0.60      0.55         5\n",
      "transmutation       0.50      0.50      0.50        12\n",
      "\n",
      "     accuracy                           0.56        64\n",
      "    macro avg       0.63      0.57      0.57        64\n",
      " weighted avg       0.60      0.56      0.56        64\n",
      "\n",
      "==================================================\n",
      "Training Model: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision = 0.4270\n",
      "Macro Recall    = 0.4292\n",
      "Macro F1-score  = 0.4255\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.25      0.25      0.25         8\n",
      "  conjuration       0.31      0.40      0.35        10\n",
      "   divination       0.00      0.00      0.00         6\n",
      "  enchantment       0.80      0.67      0.73         6\n",
      "    evocation       0.31      0.42      0.36        12\n",
      "     illusion       0.60      0.60      0.60         5\n",
      "   necromancy       0.60      0.60      0.60         5\n",
      "transmutation       0.55      0.50      0.52        12\n",
      "\n",
      "     accuracy                           0.42        64\n",
      "    macro avg       0.43      0.43      0.43        64\n",
      " weighted avg       0.41      0.42      0.41        64\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "5ee021b4",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "#### Summary Table (Macro F1)\n",
    "| Model | Macro F1 |\n",
    "|--------|-----------|\n",
    "| **RBF SVM** | **0.6458** |\n",
    "| **KNN (k=5)** | **0.6168** |\n",
    "| **Linear SVM** | **0.6108** |\n",
    "| Logistic Regression | 0.5621 |\n",
    "| Extra Trees | 0.5717 |\n",
    "| Gaussian NB | 0.5730 |\n",
    "| Polynomial SVM (deg=3) | 0.5357 |\n",
    "| Gradient Boosting | 0.3702 |\n",
    "| XGBoost | 0.4255 |\n",
    "| Decision Tree | 0.3178 |\n",
    "\n",
    "#### Key Insights\n",
    "1. **Kernel SVMs outperform all other baselines**, with RBF SVM achieving the highest macro F1.\n",
    "2. **KNN is surprisingly strong**, suggesting that spells cluster cleanly in embedding space.\n",
    "3. **Linear SVM > Logistic Regression**, confirming mild linear separability.\n",
    "4. **Tree-based models consistently underperform**; dense embeddings are not well suited for axis-aligned splits.\n",
    "5. Certain schools, especially **Divination and Transmutation**, remain challenging due to semantic overlap.\n",
    "\n",
    "#### What This Means\n",
    "- The embedding space captures semantic structure, but *not all schools are equally separable*.\n",
    "- Nonlinear decision boundaries (RBF SVM) help significantly.\n",
    "- More advanced models must focus on better representing nuanced spell semantics.\n",
    "\n",
    "#### Next Steps\n",
    "1. **Tune the RBF SVM**  \n",
    "   - Grid search `C` and `gamma`  \n",
    "   - Expect significant improvement.\n",
    "\n",
    "2. **Upgrade the embedding model**  \n",
    "   - Try `all-mpnet-base-v2` or `bge-large-en-v1.5`\n",
    "   - Strongly boosts class separation.\n",
    "\n",
    "3. **Add structured features**  \n",
    "   Combine embeddings with:\n",
    "   - spell level  \n",
    "   - casting time category  \n",
    "   - duration category  \n",
    "\n",
    "4. **Try a simple neural classifier**  \n",
    "   - MLP on top of embeddings (1–2 layers)\n",
    "\n",
    "5. **Analyze confusion matrix**  \n",
    "   - Understand which schools consistently overlap (likely: Evocation/Transmutation/Conjuration).\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-06T03:50:31.903981Z",
     "start_time": "2025-12-06T03:50:30.310437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 1. Tune the RBF SVM\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 'scale', 'auto']\n",
    "}\n",
    "\n",
    "rbf_svm = SVC(kernel='rbf')\n",
    "\n",
    "# Grid search with 5-fold CV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rbf_svm,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV macro F1:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "best_rbf_svm = grid_search.best_estimator_\n",
    "preds = best_rbf_svm.predict(X_test)\n",
    "\n",
    "macro_precision = precision_score(y_test, preds, average=\"macro\")\n",
    "macro_recall = recall_score(y_test, preds, average=\"macro\")\n",
    "macro_f1 = f1_score(y_test, preds, average=\"macro\")\n",
    "\n",
    "print(f\"Test Macro Precision = {macro_precision:.4f}\")\n",
    "print(f\"Test Macro Recall    = {macro_recall:.4f}\")\n",
    "print(f\"Test Macro F1-score  = {macro_f1:.4f}\")\n",
    "\n",
    "print(\"\\nPer-class performance:\")\n",
    "print(classification_report(y_test, preds, target_names=label_encoder.classes_))"
   ],
   "id": "9d63d8ff6c37df66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'gamma': 1}\n",
      "Best CV macro F1: 0.6407292517069236\n",
      "Test Macro Precision = 0.6771\n",
      "Test Macro Recall    = 0.6219\n",
      "Test Macro F1-score  = 0.6394\n",
      "\n",
      "Per-class performance:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   abjuration       0.71      0.62      0.67         8\n",
      "  conjuration       0.78      0.70      0.74        10\n",
      "   divination       0.67      0.33      0.44         6\n",
      "  enchantment       0.83      0.83      0.83         6\n",
      "    evocation       0.50      0.58      0.54        12\n",
      "     illusion       0.80      0.80      0.80         5\n",
      "   necromancy       0.75      0.60      0.67         5\n",
      "transmutation       0.38      0.50      0.43        12\n",
      "\n",
      "     accuracy                           0.61        64\n",
      "    macro avg       0.68      0.62      0.64        64\n",
      " weighted avg       0.64      0.61      0.61        64\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-06T03:50:31.931158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 2. Upgrade the embedding model —— all-mpnet-base-v2 & bge-large-en-v1.5\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "embedding_models = {\n",
    "    \"all-mpnet-base-v2\": 768,\n",
    "    \"BAAI/bge-large-en-v1.5\": 1024\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, expected_dim in embedding_models.items():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"Loading & encoding with → {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    X_emb = model.encode(\n",
    "        df[\"full_text\"].tolist(),\n",
    "        batch_size=32 if \"bge-large\" in model_name else 64,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    print(f\"Embedding shape: {X_emb.shape}\")\n",
    "\n",
    "    X_train_tmp, X_test_tmp, y_train_tmp, y_test_tmp = train_test_split(\n",
    "        X_emb, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    svm = SVC(kernel='rbf', random_state=42)\n",
    "    svm.fit(X_train_tmp, y_train_tmp)\n",
    "    preds = svm.predict(X_test_tmp)\n",
    "\n",
    "    macro_f1 = f1_score(y_test_tmp, preds, average='macro')\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Dim\": expected_dim,\n",
    "        \"Macro F1\": macro_f1,\n",
    "        \"Accuracy\": (preds == y_test_tmp).mean()\n",
    "    })\n",
    "\n",
    "    print(f\"Macro F1 = {macro_f1:.4f}   |   Accuracy = {(preds == y_test_tmp).mean():.4f}\")\n",
    "    print(\"\\nPer-class report:\")\n",
    "    print(classification_report(y_test_tmp, preds, target_names=label_encoder.classes_, digits=3))\n",
    "\n",
    "print(\"SUMMARY: Embedding Model Comparison\")\n",
    "summary_df = pd.DataFrame(results)\n",
    "summary_df = summary_df.sort_values(\"Macro F1\", ascending=False).reset_index(drop=True)\n",
    "summary_df.index += 1\n",
    "print(summary_df.round(4).to_string(index=True))\n",
    "\n",
    "# Save the current best embedding as X_upgraded (defaulting to the highest one from Macro F1).\n",
    "best_model_name = summary_df.iloc[0][\"Model\"]\n",
    "print(f\"\\nBest embedding model → {best_model_name}\")\n",
    "\n",
    "best_model = SentenceTransformer(best_model_name)\n",
    "X_upgraded = best_model.encode(\n",
    "    df[\"full_text\"].tolist(),\n",
    "    batch_size=32 if \"bge-large\" in best_model_name else 64,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "X_train_up, X_test_up, y_train_up, y_test_up = train_test_split(\n",
    "    X_upgraded, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nX_upgraded: {X_upgraded.shape[1]}\")"
   ],
   "id": "c14dbdb49e5a5005",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Loading & encoding with → all-mpnet-base-v2\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "887fd8f60082495ab41b4799c049937e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### 3. Add structured features\n",
    "\n",
    "df['casting_time_cat'] = df['casting_time'].str.lower().str.extract(r'(\\d+ \\w+|\\w+)')[0]\n",
    "df['casting_time_cat'] = df['casting_time_cat'].fillna('unknown').astype('category')\n",
    "\n",
    "df['duration_cat'] = df['duration'].str.lower()\n",
    "df['duration_cat'] = df['duration_cat'].str.replace('concentration, up to ', 'concentration ')\n",
    "df['duration_cat'] = df['duration_cat'].str.extract(r'(\\w+ \\w+|\\w+)')[0]\n",
    "df['duration_cat'] = df['duration_cat'].fillna('unknown').astype('category')\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "struct_features = encoder.fit_transform(df[['casting_time_cat', 'duration_cat']])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "level_scaled = scaler.fit_transform(df[['level']])\n",
    "\n",
    "X_struct = np.hstack([level_scaled, struct_features])\n",
    "\n",
    "X_combined = np.hstack([X_upgraded, X_struct])\n",
    "\n",
    "X_train_comb, X_test_comb, y_train_comb, y_test_comb = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Test the improvement on RBF SVM\n",
    "rbf_svm_comb = SVC(kernel='rbf')\n",
    "rbf_svm_comb.fit(X_train_comb, y_train_comb)\n",
    "preds_comb = rbf_svm_comb.predict(X_test_comb)\n",
    "\n",
    "macro_f1_comb = f1_score(y_test_comb, preds_comb, average=\"macro\")\n",
    "print(f\"Combined Features Macro F1 (RBF SVM): {macro_f1_comb:.4f}\")\n",
    "\n",
    "print(\"\\nPer-class performance:\")\n",
    "print(classification_report(y_test_comb, preds_comb, target_names=label_encoder.classes_))"
   ],
   "id": "51cc53511c30622"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### 4. Try a simple neural classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# 定义 MLP：2 层隐藏，100 和 50 神经元\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "mlp.fit(X_train_comb, y_train_comb)\n",
    "preds_mlp = mlp.predict(X_test_comb)\n",
    "\n",
    "macro_f1_mlp = f1_score(y_test_comb, preds_mlp, average=\"macro\")\n",
    "print(f\"MLP Macro F1: {macro_f1_mlp:.4f}\")\n",
    "\n",
    "print(\"\\nPer-class performance:\")\n",
    "print(classification_report(y_test_comb, preds_mlp, target_names=label_encoder.classes_))"
   ],
   "id": "c8eada3544fc7eee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### 5. Analyze confusion matrix\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_test_comb, preds_mlp)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop misclassifications:\")\n",
    "for i in range(len(label_encoder.classes_)):\n",
    "    for j in range(len(label_encoder.classes_)):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            print(f\"{label_encoder.classes_[i]} misclassified as {label_encoder.classes_[j]}: {cm[i, j]} times\")\n",
    "\n",
    "overlap_schools = ['evocation', 'transmutation', 'conjuration']\n",
    "overlap_indices = [np.where(label_encoder.classes_ == school)[0][0] for school in overlap_schools]\n",
    "\n",
    "print(\"\\nOverlap among Evocation/Transmutation/Conjuration:\")\n",
    "sub_cm = cm[np.ix_(overlap_indices, overlap_indices)]\n",
    "sns.heatmap(\n",
    "    sub_cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Reds',\n",
    "    xticklabels=overlap_schools,\n",
    "    yticklabels=overlap_schools\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Subset Confusion Matrix')\n",
    "plt.show()"
   ],
   "id": "b2afbe7849a2e9d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "11712490e23d7f69"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
